{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=open('/home/maria/andrej-karpathy-experiments/names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(words):\n",
    "    # create the training set of bigrams (x,y)\n",
    "    xs, ys = [], []\n",
    "    for w in words:\n",
    "      chs = ['.'] + list(w) + ['.']\n",
    "      for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "    xs = torch.tensor(xs).to('cuda')\n",
    "    ys = torch.tensor(ys).to('cuda')\n",
    "    return xs, ys\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "xs, ys = generate_bigrams(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228146"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7073, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.7072, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.7063, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.7026, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.8166, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(3.1587, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(3.2795, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Network\n",
    "generator=torch.Generator(device='cuda').manual_seed(1337)\n",
    "def nn_forward_pass(x, W):\n",
    "    x_enc=torch.nn.functional.one_hot(x,len(stoi)).float()\n",
    "    logits=x_enc@W\n",
    "    probs=logits.exp()/logits.exp().sum(1,keepdim=True)\n",
    "    return probs\n",
    "\n",
    "def loss(probs, y, alpha, W):\n",
    "    nll=-probs[range(len(y)),y].log().mean()+alpha*(W**2).mean()\n",
    "    return nll\n",
    "\n",
    "def optimize_single_hyperparam(alpha):\n",
    "    train_x=xs[:int(xs.shape[0]*0.8)]\n",
    "    train_y=ys[:int(ys.shape[0]*0.8)]\n",
    "    valid_x=xs[int(xs.shape[0]*0.8):]\n",
    "    valid_y=ys[int(ys.shape[0]*0.8):]\n",
    "    generator=torch.Generator(device='cuda').manual_seed(1337)\n",
    "    #parameters\n",
    "    W=torch.randn((len(stoi),len(stoi)),requires_grad=True, generator=generator, device='cuda')\n",
    "    for i in range(1000):\n",
    "        probs=nn_forward_pass(train_x,W)\n",
    "        nll=loss(probs,train_y, alpha,W)\n",
    "        W.grad=None\n",
    "        nll.backward()\n",
    "        W.data-=W.grad*1.0\n",
    "    validation_loss=loss(nn_forward_pass(valid_x,W),valid_y, 0, W)\n",
    "    return validation_loss\n",
    "\n",
    "\n",
    "alpha=[0.0001,0.001,0.01,0.1,1,10,100]\n",
    "\n",
    "for a in alpha:\n",
    "    print(optimize_single_hyperparam(a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
